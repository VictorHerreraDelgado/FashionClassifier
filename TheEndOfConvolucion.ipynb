{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio de redes neuronales profundas para la clasificación y caracterización de prendas de vestir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Víctor Herrera Delgado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código dispuesto a continuación consiste en una versión limpia de los pasos que realicé para el desarrollo de mi trabajo de fin de título para la clasificación y caracterización de prendas de ropa y elementos adicionales del mundo de la moda. \n",
    "\n",
    "Este proyecto te puede servir como introducción a la hora de crear tu propia red neuronal para clasificar.\n",
    "\n",
    "En concreto en este documento tratamos el uso de redes neuronales convolucionales que siguen la estructura VGG, usando como herramienta Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del código\n",
    "\n",
    "El código se divide en dos partes:\n",
    "\n",
    "- Inicialización de las clases, funciones y variables que se utilizarán. En cada bloque se explicará en que consiste cada uno y su funcionamiento.\n",
    "- Ejecución del entrenamiento y de otras fases para el análisis de los resultados.\n",
    "- Optimización de hiperparámetros.\n",
    "- Lectura avanzada de resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización del proyecto \n",
    "#### (Ejecutar todos los bloques hasta la siguiente mitad para el uso directo del código)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Importación de librería e inicialización de variables globales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import skopt\n",
    "from skopt import callbacks\n",
    "from skopt import dump, load\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "import math\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from livelossplot import PlotLosses\n",
    "import sys\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "class Mode(Enum):\n",
    "    GENERAL = 0  #Modelo modificable por el usuario\n",
    "    VGG = 1      #Uso único de la clasificación de VGG\n",
    "    MIDVGG3 = 2  #Uso único de fine-tunning de 3 capas de VGG\n",
    "    BLANKVGG = 3 #Red con estructura VGG normal\n",
    "    MIDVGG2 = 4  #Uso único de fine-tunning de 2 capas de VGG\n",
    "    MIDVGG1 = 5  #Uso único de fine-tunning de 1 capa de VGG\n",
    "\n",
    "MODE = Mode.VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de imágenes\n",
    "\n",
    "Si los datos se toman directamente desde el dataset, la variable \"transform\" permitirá adaptar los datos normalizados para la entrada a las capas tipo VGG.\n",
    "Si se usa la variable \"dataAugmentationTransform\", los datos se rotaran, daran la vuelta y recortaran aleatoriamente para aumentar la cantidad de datos que componen el dataset durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/storage/datasets/fashion-dataset/\" \n",
    "img_width, img_height, _ = 32, 32, 3\n",
    "image_size = (img_width, img_height)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "#mean = [0.8553, 0.8376, 0.8319]\n",
    "#std = [0.2711, 0.2837, 0.2879]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "dataAugmentationTransform = transforms.Compose([\n",
    "    transforms.Resize(235),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20, resample=False, expand=False, center=None),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "El dataset se guarda en un objeto del tipo dataset y cuenta con el siguiente grupo de variables:\n",
    "- *path*: El path del dataset original.\n",
    "- *file_list*: la lista de imágenes que se pueden leer.\n",
    "- *division*: la categoría por la que se dividen.\n",
    "- *transform*: el transformador que actua sobre sus imágenes.\n",
    "- *chargedInVgg*: boolean para indicar si salen de el las imágenes o los resultados de pasar las imágenes por la parte convolutiva.\n",
    "- *PreResults*: resultados de pasar las imágenes por la parte convolutiva.\n",
    "El dataset se inicia pasandole un objeto transform y el campo por el que se clasificarán. Al cargarlo, se comprobará la fiabilidad de las imágenes.  \n",
    "Para obtener los resultados de pasarla por la parte convolutiva de la VGG (o por una parte de la red) hay que ejecutar la función preNet de la clase, que pasándole la parte convolutiva de la red se encargará de ello. Podemos con la función changeSource(bool) especificar que valores queremos obtener del dataset, si las imágenes (bool = False) o los resultados de pasar por la parte de la red con anterioridad (bool = True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Recuerde hacer en la función \"preparationForPreConvolutive\" el cambio correspondiente de ficheros para adaptarla a usted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheDataset(Dataset):\n",
    "    def __init__(self,transform=None,division=None):\n",
    "        self.path= DATA_PATH\n",
    "        self.file_list = pd.read_csv(self.path + \"styles.csv\", error_bad_lines=False)\n",
    "        self.file_list['image'] =self.file_list.apply(lambda row: self.path + \"images/\"+ str(row['id']) + \".jpg\", axis=1)\n",
    "        self.file_list = self.file_list.reset_index(drop=True)\n",
    "        self.division = division\n",
    "        self.classes= tuple(self.file_list[division].unique())\n",
    "        self.transform = transform\n",
    "        self.tryDataset()\n",
    "        self.chargedInVGG = 0\n",
    "        self.PreResults = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if self.chargedInVGG==Mode.GENERAL or self.chargedInVGG==Mode.BLANKVGG:\n",
    "            row = self.file_list[idx]\n",
    "            imageClass = self.classes.index(row[self.division])\n",
    "            image = Image.open(row['image'])\n",
    "            if self.transform:\n",
    "                    image= self.transform(image)\n",
    "            return image, imageClass \n",
    "        else:\n",
    "            row = self.file_list[idx]\n",
    "            imageClass = self.classes.index(row[self.division])\n",
    "            if self.chargedInVGG == Mode.MIDVGG3:\n",
    "                return  torch.load(row['midConvolutive3']), imageClass\n",
    "            elif self.chargedInVGG == Mode.MIDVGG2:\n",
    "                return  torch.load(row['midConvolutive2']), imageClass\n",
    "            elif self.chargedInVGG == Mode.MIDVGG1:\n",
    "                return  torch.load(row['midConvolutive1']), imageClass\n",
    "            elif self.chargedInVGG == Mode.VGG:\n",
    "                return torch.load(row['completeConvolutive']), imageClass\n",
    "    \n",
    "    def tryDataset(self):\n",
    "        cont = 0\n",
    "        files=[]\n",
    "        for index, row in self.file_list.iterrows():\n",
    "            if not os.path.isfile(row['image']) : continue\n",
    "            if row['masterCategory'] == \"Free Items\" or row['subCategory'] == \"Free Gifts\" or row['articleType'] == \"Free Gifts\" or (row['baseColour'] == \"nan\") or str(row['season']) == \"nan\" or str(row['usage']) == \"nan\":\n",
    "                continue\n",
    "            files.append(row)\n",
    "            cont += 1\n",
    "    \n",
    "\n",
    "        self.file_list = files\n",
    "    def changeSource(self,mode):\n",
    "        self.chargedInVGG = mode\n",
    "    \n",
    "    def preparationForPreConvolutive(self,mode):\n",
    "        if mode == Mode.MIDVGG3:\n",
    "            for index in range(len(self.file_list)):\n",
    "                self.file_list[index]['midConvolutive3'] = \"/storage/midConvolutiveResults3_2/file\" + str(self.file_list[index]['id']);\n",
    "        elif mode == Mode.MIDVGG2:\n",
    "            for index in range(len(self.file_list)):\n",
    "                self.file_list[index]['midConvolutive2'] = \"/storage/midConvolutiveResults2_2/file\" + str(self.file_list[index]['id']);\n",
    "        elif mode == Mode.MIDVGG1:\n",
    "            for index in range(len(self.file_list)):\n",
    "                self.file_list[index]['midConvolutive1'] = \"/storage/midConvolutiveResults1_2/file\" + str(self.file_list[index]['id']);\n",
    "        elif mode == Mode.VGG:\n",
    "            for index in range(len(self.file_list)):\n",
    "                self.file_list[index]['completeConvolutive'] = \"/storage/convolutiveResultsNoFree_2/file\" + str(self.file_list[index]['id']);\n",
    "   \n",
    "    def completePreNet(self, notClassNet,folderPath):\n",
    "        #DATA_PATH = \"/storage/datasets/fashion-dataset/\"\n",
    "        files = []\n",
    "        count = 0\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "        notClassNet.to(device)\n",
    "        notClassNet.cuda()\n",
    "        with torch.no_grad():\n",
    "            for index in range(len(self.file_list)):\n",
    "                if index % 5000 == 0: \n",
    "                    print(index)\n",
    "                imgAndClass = []\n",
    "                theImg,theClass = self.__getitem__(index)\n",
    "                if(theClass == 4):\n",
    "                    print(self.file_list[index])\n",
    "                theImg = torch.tensor([theImg.tolist()])\n",
    "                theImg = theImg.to(device) \n",
    "                output = notClassNet(theImg)\n",
    "                output = torch.flatten(output, 1)\n",
    "                output = torch.tensor(output.tolist()[0])\n",
    "                torch.save(output,folderPath + \"file\" + str(self.file_list[index]['id']) );\n",
    "                self.file_list[index]['completeConvolutive'] = folderPath +\"file\" + str(self.file_list[index]['id']);\n",
    "                torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    def midPreNet(self, notClassNet,folderPath):\n",
    "        count = 0\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "        notClassNet.to(device)\n",
    "        notClassNet.cuda()\n",
    "        with torch.no_grad():\n",
    "            for index in range(len(self.file_list)):\n",
    "                if index % 5000 == 0: \n",
    "                    print(index)\n",
    "                \n",
    "                imgAndClass = []\n",
    "                theImg,theClass = self.__getitem__(index)\n",
    "                theImg = torch.tensor([theImg.tolist()])\n",
    "                theImg = theImg.to(device) \n",
    "                output = notClassNet(theImg)\n",
    "                output = torch.tensor(output.tolist()[0])\n",
    "                torch.save(output,folderPath + \"file\" + str(self.file_list[index]['id']) );\n",
    "                self.file_list[index]['midConvolutive'] = folderPath + \"file\" + str(self.file_list[index]['id']);\n",
    "                torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "#### Ejemplo de creación\n",
    "El dataloader será el que proporcionará los distintos elementos del dataset al modelo de la red, además de dividirlos en entrenamiento y validación y configurar los parámetros de tamaño de batch y la aleatoriedad de los elementos del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATALOADER\n",
    "def createDataloader(batch_size = 4, num_workers = 0, trainSize = 30000):\n",
    "    shuffle = False\n",
    "    valSize = len(theDatasetPr)-trainSize\n",
    "    testLoader = DataLoader(dataset=test_ds,\n",
    "                            shuffle=shuffle,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers)\n",
    "    validLoader = DataLoader(dataset=valid_ds,\n",
    "                            shuffle=shuffle,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers)\n",
    "    print(validLoader.batch_size)\n",
    "    dataloaders = {'train': testLoader, 'val': validLoader}\n",
    "    dataset_sizes = {'train': trainSize, 'val': valSize}\n",
    "    return dataloaders,dataset_sizes\n",
    "#def resetNetModel():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error de entrenamiento\n",
    "### Ajuste del error de entrenamiento a tener en cuenta\n",
    "En función del modelo que se le pase, se obtendrán los parámetros de la obtención del error en función de los cuales se corregirá el modelo para que cumpla el objetivo propuesto. A la función se le da como parámetros el modelo a entrenar y el learning rate deseado (0.001 por defecto). Devuelve las variables Criterion, el optimizador y el scheduler.\n",
    "\n",
    "Se utiliza para la corrección el descenso por el gradiente estocástico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingError(model,lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer= optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    return criterion,optimizer,exp_lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "Dependiendo de la elección realizada (tipo de red a utilizar) se utilizará una configuración u otra. Las configuraciones son modificables pero en general son:\n",
    "- General: Se tiene una red simple \"GeneralModel\" que puede ser modificable por el usuario.\n",
    "- BLANKVGG: Devuelve una estructura VGG para su entrenamiento.\n",
    "- VGG: Una red basada en una VGG preentrenada y a la cual se le puede modificar su parte clasificadora. Se puede modificar en \"VGGModel\". \n",
    "- MidVGG1, MidVGG2, MidVGG3: Una red basada en la red obtenida al crear un modelo en VGG con la diferencia de que parte de sus capas convolutivas se entrenan, véase que se realiza fine-tunning. Se puede modificar la parte convolutiva seleccionada en \"MidVGGModel\". El número que sigue a MIDVGG depende del número de capas convolucionales que se modifiquen.\n",
    "\n",
    "##### (Por defecto VGG y las MIDVGG están preparadas para entradas de datos ya preprocesadas por el dataset para disminuir su coste. Si desea entrenarla de forma convencional descomente los comentarios en sus respectivas funciones forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifier(intro,outro,netArray,d_dropout=0.5):\n",
    "    if(len(netArray) < 1): \n",
    "        return []\n",
    "    newClassifier = [nn.Linear(intro, netArray[0]),nn.Dropout(p=d_dropout, inplace=False),nn.ReLU(inplace=True)]\n",
    "    for i in range(len(netArray)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        else:  \n",
    "            newClassifier = newClassifier + [nn.Linear(netArray[i-1], netArray[i]),nn.Dropout(p=d_dropout, inplace=False),nn.ReLU(inplace=True)]\n",
    "    newClassifier = newClassifier + [nn.Linear(netArray[len(netArray) - 1], outro)]\n",
    "    return newClassifier        \n",
    "    \n",
    "class GeneralModel (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3)\n",
    "        #30x30x6\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #15x15x6\n",
    "        self.conv2 = nn.Conv2d(6,16,2)\n",
    "        #14x14x16\n",
    "        self.conv3 = nn.Conv2d(16,32,5)\n",
    "        #10x10x16\n",
    "        self.fc1 = nn.Linear(32*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,len(theDatasetPr.classes))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1,32*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class VGGModel (nn.Module):\n",
    "    def __init__(self,net,dropout,netArray):\n",
    "        super(VGGModel,self).__init__()\n",
    "        self.features = net.features\n",
    "        for param in self.features.parameters():\n",
    "            param.require_grad = False\n",
    "        nFeats = net.classifier[6].in_features\n",
    "        feats = list(net.classifier.children())[:-7]\n",
    "        nFeats = net.classifier[0].in_features\n",
    "        feats.extend(getClassifier(nFeats,len(theDatasetPr.classes),netArray,dropout))\n",
    "        self.avgpool = net.avgpool\n",
    "        self.classifier = nn.Sequential(*feats)\n",
    "    def forward(self,x):\n",
    "        #x = self.features(x)\n",
    "        #x = self.avgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x    \n",
    "\n",
    "class MidVGG3Model (nn.Module):\n",
    "    def __init__(self,net):\n",
    "        super(MidVGG3Model,self).__init__()\n",
    "        self.features = net.features[:-10]\n",
    "        self.newFeatures = net.features[-10:]\n",
    "        for param in self.newFeatures.parameters():\n",
    "            param.require_grad = True\n",
    "        self.avgpool = net.avgpool\n",
    "        self.classifier = net.classifier\n",
    "    def forward(self,x):\n",
    "        #x = self.features(x)\n",
    "        x = self.newFeatures(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class MidVGG2Model (nn.Module):\n",
    "    def __init__(self,net):\n",
    "        super(MidVGG2Model,self).__init__()\n",
    "        self.features = net.features[:-7]\n",
    "        self.newFeatures = net.features[-7:]\n",
    "        for param in self.newFeatures.parameters():\n",
    "            param.require_grad = True\n",
    "        self.avgpool = net.avgpool\n",
    "        self.classifier = net.classifier\n",
    "    def forward(self,x):\n",
    "        #x = self.features(x)\n",
    "        x = self.newFeatures(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class MidVGG1Model (nn.Module):\n",
    "    def __init__(self,net):\n",
    "        super(MidVGG1Model,self).__init__()\n",
    "        self.features = net.features[:-4]\n",
    "        self.newFeatures = net.features[-4:]\n",
    "        for param in self.newFeatures.parameters():\n",
    "            param.require_grad = True\n",
    "        self.avgpool = net.avgpool\n",
    "        self.classifier = net.classifier\n",
    "    def forward(self,x):\n",
    "        #x = self.features(x)\n",
    "        x = self.newFeatures(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "def generateModel(MODE,dropout,netArray):\n",
    "    newModel = None\n",
    "    print(MODE)\n",
    "    if(MODE == Mode.GENERAL):\n",
    "        newModel = GeneralModel()\n",
    "        return newModel\n",
    "    if(MODE == Mode.VGG):\n",
    "        newModel = models.vgg16_bn(pretrained=True)\n",
    "        newModel = VGGModel(newModel,dropout,netArray)\n",
    "        return newModel\n",
    "    if(MODE == Mode.BLANKVGG):\n",
    "        newModel = models.vgg16_bn(pretrained=True)\n",
    "        feats = list(newModel.classifier.children())[:-1]\n",
    "        feats.extend([nn.Linear(4096, len(theDatasetPr.classes))])\n",
    "        newModel.classifier = nn.Sequential(*feats)\n",
    "        return newModel\n",
    "    if(MODE == Mode.MIDVGG3):\n",
    "        newModel = models.vgg16_bn(pretrained=True)\n",
    "        MODE = Mode.VGG\n",
    "        newModel = MidVGG3Model(generateModel(MODE,dropout,netArray))\n",
    "        MODE = Mode.MIDVGG3\n",
    "        return newModel\n",
    "    if(MODE == Mode.MIDVGG2):\n",
    "        newModel = models.vgg16_bn(pretrained=True)\n",
    "        MODE = Mode.VGG\n",
    "        newModel = MidVGG2Model(generateModel(MODE,dropout,netArray))\n",
    "        MODE = Mode.MIDVGG2\n",
    "        return newModel\n",
    "    if(MODE == Mode.MIDVGG1):\n",
    "        newModel = models.vgg16_bn(pretrained=True)\n",
    "        MODE = Mode.VGG\n",
    "        newModel = MidVGG1Model(generateModel(MODE,dropout,netArray))\n",
    "        MODE = Mode.MIDVGG1\n",
    "        return newModel\n",
    "    return newModel\n",
    "    \n",
    "def createModel(MODE=Mode.GENERAL,dropout=0.5,netArray=[4096,4096]):\n",
    "    newModel = generateModel(MODE,dropout,netArray)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    newModel.to(device)\n",
    "    newModel.cuda()\n",
    "    return newModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red\n",
    "Pasándole los parámetros necesarios realizamos un entrenamiento en el cual para cada época se entrena y se valida la red, mostrando los resultados de pérdida y precisión para cada caso. Se ha incorporado la librería liveloss para poder mostrar estos resultados como gráficas de los resultados a tiempo real. \n",
    "La función guardará la red que mejores resultados haya obtenido y no cambiará hasta encontrar una que mejore su ejecución en la validación.\n",
    "Al finalizar, devuelve el modelo más óptimo obtenido, así como la media de las perdidas obtenidas, que tiene su uso a la hora de buscar los hiperparámetros óptimos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función adicional para imprimir el resultado en el fichero especificado aquí\n",
    "def printInFile(time_elapsed, best_acc, the_real_acc, the_loss,the_train_acc,the_train_loss):\n",
    "    print(\"ENTRAMOS\")\n",
    "    with open('LASTRESULTS.txt', 'w') as f:\n",
    "        print(\"TRAINING\")\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60),file=f)\n",
    "        print('Best val Acc: {:4f}'.format(best_acc),file=f)\n",
    "        \n",
    "        print(\"ACCURACY\")\n",
    "        print(\"VALIDATION\",file=f)\n",
    "        print(\"ACCURACY HISTORY\",file=f)\n",
    "        for i in the_real_acc: \n",
    "            print(i.item(),file=f) \n",
    "        print(\"\\n\\nLOSS HISTORY\",file=f)\n",
    "        for x in the_loss: \n",
    "            print(x,file=f) \n",
    "            \n",
    "        print(\"\\n\\nTRAINING\",file=f)\n",
    "        print(\"ACCURACY HISTORY\",file=f)\n",
    "        for i in the_train_acc: \n",
    "            print(i.item(),file=f) \n",
    "        print(\"\\nLOSS HISTORY\",file=f)\n",
    "        for x in the_train_loss: \n",
    "            print(x,file=f) \n",
    "        print(\"FIN\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basado en la documentación de pytorch\n",
    "def train_model(model, thecriterion, theoptimizer, thescheduler,dataloaders=None,dataset_sizes=None, num_epochs=25):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    since = time.time()\n",
    "    liveloss = PlotLosses()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 1000000.0\n",
    "    the_loss = []\n",
    "    the_acc = []\n",
    "    the_real_acc = []\n",
    "    the_train_acc = []\n",
    "    the_train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        logs={}\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            batchCont = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                theoptimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = thecriterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        theoptimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batchCont+=1\n",
    "                if batchCont % 250 == 0: \n",
    "                    print(\"batch = \",batchCont)\n",
    "            if phase == 'train':\n",
    "                thescheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss\n",
    "            logs[prefix + 'accuracy'] = epoch_acc\n",
    "\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), \"./mejorActual\")\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                #torch.save(model.state_dict(), \"./mejorActual\")\n",
    "            if phase == 'val':\n",
    "                the_acc.append((1.0 - epoch_acc).double())\n",
    "                the_real_acc.append(epoch_acc)\n",
    "                the_loss.append(epoch_loss)\n",
    "                liveloss.update(logs)\n",
    "                liveloss.send()\n",
    "            if phase == 'train':\n",
    "                the_train_acc.append(epoch_acc)\n",
    "                the_train_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    printInFile(time_elapsed, best_acc, the_real_acc, the_loss, the_train_acc,the_train_loss)\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudios individuales\n",
    "A continuación se exponen las funciones dedicadas a observar la sensibilidad de un modelo para una red entrenada, obteniendo sus resultados de precisión para cada categoría en el conjunto de categorías actual en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_success_val(model,dataloaders):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #netPr.to(device)\n",
    "    lengu = len(theDatasetPr.classes)\n",
    "    class_correct = list(0. for i in range(lengu))\n",
    "    class_total = list(0. for i in range (lengu))\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['val']:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs,1);\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                #if(len(labels) < 4): continue\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] +=1\n",
    "\n",
    "    #for i in range(lengu):\n",
    "    #  print('Accuracy of %5s : %2d %%' % (\n",
    "    #      theDatasetPr.classes[i],100 * class_correct[i]/(max(class_total[i],1))), \"Total = \", class_total[i])\n",
    "    return class_correct,class_total\n",
    "\n",
    "def class_success_train(model,dataloaders):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #netPr.to(device)\n",
    "    lengu = len(theDatasetPr.classes)\n",
    "    class_correct = list(0. for i in range(lengu))\n",
    "    class_total = list(0. for i in range (lengu))\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['train']:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs,1);\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                #if(len(labels) < 4): continue\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] +=1\n",
    "\n",
    "    #for i in range(lengu):\n",
    "    #  print('Accuracy of %5s : %2d %%' % (\n",
    "    #      theDatasetPr.classes[i],100 * class_correct[i]/(max(class_total[i],1))), \"Total = \", class_total[i])\n",
    "    return class_correct, class_total\n",
    "\n",
    "def class_success_total(dataloaders):\n",
    "    lengu = len(theDatasetPr.classes)\n",
    "    class_total = list(0. for i in range (lengu))\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['train']:\n",
    "            _, labels = data\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_total[label] +=1\n",
    "        for data in dataloaders['val']:\n",
    "            _, labels = data\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_total[label] +=1\n",
    "    #for i in range(lengu):\n",
    "    #    print(theDatasetPr.classes[i],\"    Total = \", class_total[i])\n",
    "    return class_total\n",
    "\n",
    "def class_success(model,dataloaders):\n",
    "    correctVal, totalVal = class_success_val(model,dataloaders)#createDataloader(batch_size = 64, num_workers = 0, trainSize = 30000))\n",
    "    print(\" \")\n",
    "    correctTrain, totalTrain = class_success_train(model,dataloaders) #createDataloader(batch_size = 64, num_workers = 0, trainSize = 30000))\n",
    "    print(\" \")\n",
    "    totalClass = class_success_total(dataloaders)#createDataloader(batch_size = 64, num_workers = 0, trainSize = 30000))\n",
    "    #print(\"Clase\\t\\t\\tTotal\\t\\t\\tPrecision Val\\t\\t\\tPrecision Train\\t\\t\\tTotal Val\\t\\t\\tTotal Train\")\n",
    "    for i in range(len(theDatasetPr.classes)):\n",
    "        print(\n",
    "          theDatasetPr.classes[i], \"\\t\\t\\t\",totalClass[i] ,\n",
    "            \"\\t\\t\\n Val%\",\n",
    "            (100 * correctVal[i]/(max(totalVal[i],1))),\n",
    "            \"\\t\\t   Train%\",\n",
    "            (100 * correctTrain[i]/(max(totalTrain[i],1))),\n",
    "            \"\\t\\t   VT\",\n",
    "            totalVal[i],\n",
    "            \"\\t\\t   TT\",\n",
    "            totalTrain[i], \n",
    "            \"\\n\"\n",
    "            )\n",
    "    return totalClass,correctVal,totalVal,correctTrain,totalTrain\n",
    "def confusion_prepare(model,dataloaders):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #netPr.to(device)\n",
    "    lengu = len(dataloaders['val'])\n",
    "    print(\"lengu= \", lengu)\n",
    "    well_predicted = []#list(0. for i in range(lengu))\n",
    "    prediction = []#list(0. for i in range (lengu))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['val']:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs,1);\n",
    "            for i in range(len(labels)):\n",
    "                prediction.append(theDatasetPr.classes[predicted[i].item()])\n",
    "                well_predicted.append(theDatasetPr.classes[labels[i].item()])\n",
    "            #    label = labels[i]\n",
    "            #    class_correct[label] += c[i].item()\n",
    "            #    class_total[label] +=1\n",
    "    return well_predicted,prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Sección de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos el tipo de red que queremos entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = Mode.MIDVGG3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Preparamos el dataset pasandole el transformador adecuado y la categoría por la cual vamos a clasificar. En caso de necesitar cambiar categoría debemos volver a ejecutar este bloque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr = TheDataset(transform=transform,division=\"masterCategory\")\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta el bloque a continuación solo si quieres entrenar con datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr.changeSource(Mode.GENERAL)\n",
    "theDatasetPr.preparationForPreConvolutive(MODE)\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Preparamos los dataloader para suministrar la información. En caso de querer cambiar el tamaño de batch o la distribución de elementos cambia los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize= 30000\n",
    "shuffle = True\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "valSize = len(theDatasetPr)-trainSize\n",
    "dataloader = DataLoader(dataset=theDatasetPr,\n",
    "                            shuffle=shuffle,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (manual_seed permitirá que entrenamiento y validación se dividan siempre de la misma manera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "test_ds, valid_ds = torch.utils.data.random_split(dataloader.dataset, (trainSize, valSize))\n",
    "\n",
    "dataloads, dataset_sis = createDataloader(batch_size = batch_size, num_workers = 0, trainSize= trainSize)#trainSize = 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo y error\n",
    "\n",
    "Creación del modelo y ajuste de la corrección del error.\n",
    "\n",
    "A createModel además de pasarle el modo se le puede pasar el dropout (por defecto 0.5) aplicar en la clasificación. También se le puede pasar una tupla que contenga el valor de entrada y salida de las capas intermedias. Por ejeplo si queremos una capa de entrada 512 y salida 512 pondremos \\[512, 512\\]. En caso de querer añadir una segunda capa añada su número de salidas con otro elemento de la tupla. Por defecto tiene una capa de 4096x4096.\n",
    "\n",
    "A los parámetros del error, además del modelo creado debemos pasarle el learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNet = createModel(MODE,0.5)\n",
    "crit,opts, exp_lr_scheds = createTrainingError(classNet,lr=0.003920274771701523)\n",
    "print(classNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (En caso de querer cargar los pesos de una red anterior, puede usar la siguiente función)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNet.load_state_dict(torch.load(\"./modeloAntiguo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Para ponerla a funcionar debe pasarle los parámetros creados durante las últimas funciones y el número de épocas.\n",
    "\n",
    "Se le devolverá el modelo que mejor exactitud haya logrado y el valor de esa exactitud.\n",
    "Los mejores pesos obtenidos serán introducidos en el sistema con el nombre declarado en la función de entrenamiento.\n",
    "La evolución será impresa en el archivo especificado en la función de impresión. **Para el uso de la impresión debe borrar el archivo anterior** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNet,bestAcc = train_model(classNet, crit, opts, exp_lr_scheds,dataloads,dataset_sis,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de la sensibilidad\n",
    "\n",
    "La función a continuación le suministrará datos acerca del número de elementos de cada conjunto y la sensibilidad que logró para cada posible clasificación de la categoría.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retTotalClass,retCorrectVal,retTotalVal,retCorrectTrain,retTotalTrain = class_success(classNet,dataloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Añadido para comprobar si a sensibilidad de una clase está relacionada con el número de elementos que la componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valPercentage = []\n",
    "trainPercentage = []\n",
    "elementsTotalPercentage = []\n",
    "for i in range(len(theDatasetPr.classes)):\n",
    "    valPercentage.insert(i,100 * retCorrectVal[i]/max(retTotalVal[i],1))\n",
    "    trainPercentage.insert(i,100 * retCorrectTrain[i]/max(retTotalTrain[i],1))\n",
    "    elementsTotalPercentage.insert(i,100 * retTotalClass[i]/len(theDatasetPr))\n",
    "xyz = np.array([retTotalClass,\n",
    "                valPercentage,\n",
    "                trainPercentage\n",
    "                ])\n",
    "xyz = np.corrcoef(xyz,rowvar= True);\n",
    "print(xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización común al entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr = TheDataset(transform=transform,division=\"masterCategory\")\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr.changeSource(Mode.GENERAL)\n",
    "theDatasetPr.preparationForPreConvolutive(MODE)\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize= 30000\n",
    "shuffle = True\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "valSize = len(theDatasetPr)-trainSize\n",
    "dataloader = DataLoader(dataset=theDatasetPr,\n",
    "                            shuffle=shuffle,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers)\n",
    "torch.manual_seed(0)\n",
    "test_ds, valid_ds = torch.utils.data.random_split(dataloader.dataset, (trainSize, valSize))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de rangos\n",
    "\n",
    "Aquí se especifica los rangos en los cuales van a oscilar nuestros hiperparámetros.\n",
    "\n",
    "La variable global DaBestAcc se debe inicializar con el mínimo porcentaje no acertado de la red (1 - exactitud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = [\n",
    "    skopt.space.Real(0.001, 0.05, name='learning_rate', prior='log-uniform') #0.001\n",
    "    ,skopt.space.Categorical([16,32,64], name='batch_size' )\n",
    "    ,skopt.space.Categorical([8192,4096,2048,1024,512],name='neurons1')\n",
    "    ,skopt.space.Categorical([8192,4096,2048,1024,512],name='neurons2')\n",
    "    #,skopt.space.Real(0.1, 0.9, name='dropout') \n",
    "    #,skopt.space.Categorical([8192,4096,2048,1024,512],name='optional_neurons')\n",
    "]\n",
    "\n",
    "\n",
    "global DaBestAcc\n",
    "DaBestAcc = 0.06325128792215229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función objetivo\n",
    "\n",
    "Función objetivo la cual queremos optimizar, véase el modelo de red. Guardará temporalmente el valor mínimo obtenido en global para hacer comparaciones y salvar en el sistema aquellos modelos que hayan hecho una mejor clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.reset_max_memory_cached()\n",
    "    print(params)\n",
    "    learning_rate  = params[0]\n",
    "    the_batch_size = int(params[1])\n",
    "    neurons1 = params[2]\n",
    "    neurons2 = params[3]\n",
    "    the_dropout =  0.5#params[4];\n",
    "    optional_neurons= None\n",
    "    #optional_neurons = params[5]\n",
    "\n",
    "    dataloads, dataset_sis = createDataloader(batch_size = the_batch_size, num_workers = 0, trainSize= trainSize)#trainSize = 30000)\n",
    "    classNet = None\n",
    "    if(optional_neurons == None):\n",
    "        classNet = createModel(MODE,the_dropout,[neurons1,neurons2])\n",
    "    else:\n",
    "        classNet = createModel(MODE,the_dropout,[neurons1,neurons2,neurons3])\n",
    "    #classNet = nn.Sequential(*list(classNets.children())[2])\n",
    "    print(classNet)\n",
    "\n",
    "    \n",
    "    \n",
    "    crit,opts, exp_lr_scheds = createTrainingError(classNet,lr=learning_rate)\n",
    "    \n",
    "    classNet,loss = train_model(classNet, crit, opts, exp_lr_scheds,dataloads,dataset_sis,num_epochs=15)\n",
    "    loss = 1.0 - loss\n",
    "    print(loss)\n",
    "    global DaBestAcc\n",
    "    if (loss < DaBestAcc):\n",
    "        DaBestAcc = loss\n",
    "        print(\"NICE\",DaBestAcc )\n",
    "        torch.save(classNet.state_dict(), \"./DaBestAcc\")\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código crea un checkpoint que sirve para tener constancia del progreso de la optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_saver = CheckpointSaver(\"./newCheckpoint1.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Si es la primera vez que optimizas, ejecuta este bloque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = skopt.forest_minimize(objective, SPACE,n_calls=30,callback=[checkpoint_saver])\n",
    "dump(results, 'newHp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Si no, puedes cargar un checkpoint a partir del cual seguir buscando los mejores valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_saver = CheckpointSaver(\"./newCheckpoint1.pkl\", compress=9)\n",
    "x0 = checkRes.x_iters\n",
    "y0 = checkRes.func_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = skopt.forest_minimize(objective, SPACE,n_calls=30,x0=x0,y0=y0,callback=[checkpoint_saver])#,n_random_starts=3)\n",
    "dump(results, 'newHp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Estudio adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr = TheDataset(transform=transform,division=\"masterCategory\")\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDatasetPr.changeSource(Mode.GENERAL)\n",
    "theDatasetPr.preparationForPreConvolutive(MODE)\n",
    "theDatasetPr.changeSource(MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize= 30000\n",
    "shuffle = True\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "valSize = len(theDatasetPr)-trainSize\n",
    "dataloader = DataLoader(dataset=theDatasetPr,\n",
    "                            shuffle=shuffle,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers)\n",
    "torch.manual_seed(0)\n",
    "test_ds, valid_ds = torch.utils.data.random_split(dataloader.dataset, (trainSize, valSize))\n",
    "dataloads, dataset_sis = createDataloader(batch_size = batch_size, num_workers = 0, trainSize= trainSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNet = createModel(MODE,0.5) \n",
    "classNet.load_state_dict(torch.load(\"./modeloAntiguo\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas, prediccion = confusion_prepare(classNet,dataloads)\n",
    "print(len(etiquetas))\n",
    "print(len(prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labs = theDatasetPr.classes\n",
    "print(labs)\n",
    "print(type(labs[4]))\n",
    "labs = tuple(x for x in labs if (x != 'Free Gifts' and type(x) != type(1.1)))\n",
    "array = confusion_matrix(etiquetas, prediccion, labels=labs)\n",
    "df_cm = pd.DataFrame(array, index = labs, columns = labs)\n",
    "plt.figure(figsize = (20,15))\n",
    "sn.set(font_scale=1)\n",
    "\n",
    "heatmap = sn.heatmap(df_cm, annot=True,fmt='g')\n",
    "snsplot = heatmap.get_figure()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado opcional de la gráfica\n",
    "snsplot.savefig(\"heatMap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(etiquetas, prediccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(etiquetas, prediccion, labels=theDatasetPr.classes, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(etiquetas, prediccion, labels=theDatasetPr.classes, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
